<p align="center">
    <img src="./assets/readme/icon.png" width="250"/>
</p>
<div align="center">
    <a href="https://github.com/hpcaitech/Open-Sora/stargazers"><img src="https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social"></a>
    <a href="https://hpcaitech.github.io/Open-Sora/"><img src="https://img.shields.io/badge/Gallery-View-orange?logo=&amp"></a>
    <a href="https://discord.gg/kZakZzrSUT"><img src="https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp"></a>
    <a href="https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-247ipg9fk-KRRYmUl~u2ll2637WRURVA"><img src="https://img.shields.io/badge/Slack-ColossalAI-blueviolet?logo=slack&amp"></a>
    <a href="https://twitter.com/yangyou1991/status/1769411544083996787?s=61&t=jT0Dsx2d-MS5vS9rNM5e5g"><img src="https://img.shields.io/badge/Twitter-Discuss-blue?logo=twitter&amp"></a>
    <a href="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png"><img src="https://img.shields.io/badge/å¾®ä¿¡-å°åŠ©æ‰‹åŠ ç¾¤-green?logo=wechat&amp"></a>
    <a href="https://hpc-ai.com/blog/open-sora-v1.0"><img src="https://img.shields.io/badge/Open_Sora-Blog-blue"></a>
    <a href="https://huggingface.co/spaces/hpcai-tech/open-sora"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Gradio Demo-blue"></a>
</div>

## Open-Sora: è®©æ‰€æœ‰äººéƒ½èƒ½è½»æ¾åˆ¶ä½œé«˜æ•ˆè§†é¢‘

æˆ‘ä»¬è®¾è®¡å¹¶å®æ–½äº†**Open-Sora**ï¼Œè¿™æ˜¯ä¸€é¡¹è‡´åŠ›äºé«˜æ•ˆåˆ¶ä½œé«˜è´¨é‡è§†é¢‘çš„è®¡åˆ’ã€‚æˆ‘ä»¬å¸Œæœ›è®©æ‰€æœ‰äººéƒ½èƒ½ä½¿ç”¨æ¨¡å‹ã€å·¥å…·å’Œæ‰€æœ‰ç»†èŠ‚ã€‚é€šè¿‡é‡‡ç”¨å¼€æºåŸåˆ™ï¼ŒOpen-Sora ä¸ä»…ä½¿é«˜çº§è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„ä½¿ç”¨å˜å¾—æ°‘ä¸»åŒ–ï¼Œè€Œä¸”è¿˜æä¾›äº†ä¸€ä¸ªç®€åŒ–ä¸”ç”¨æˆ·å‹å¥½çš„å¹³å°ï¼Œç®€åŒ–äº†è§†é¢‘ç”Ÿæˆçš„å¤æ‚æ€§ã€‚å€ŸåŠ© Open-Soraï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨å†…å®¹åˆ›ä½œé¢†åŸŸä¿ƒè¿›åˆ›æ–°ã€åˆ›é€ åŠ›å’ŒåŒ…å®¹æ€§ã€‚

[[ä¸­æ–‡æ–‡æ¡£]](/docs/zh_CN/README.md) [[æ½æ™¨äº‘éƒ¨ç½²è§†é¢‘æ•™ç¨‹]](https://www.bilibili.com/video/BV141421R7Ag)

## ğŸ“° èµ„è®¯

* **[2024.06.17]** ğŸ”¥
* æˆ‘ä»¬å‘å¸ƒäº†**Open-Sora 1.2**ï¼Œå…¶ä¸­åŒ…æ‹¬**3D-VAE**ï¼Œ**æ•´æµæµ**å’Œ**å¾—åˆ†æ¡ä»¶**ã€‚è§†é¢‘è´¨é‡å¤§å¤§æé«˜ã€‚[[checkpoints]](#open-sora-10-model-weights) [[report]](/docs/report_03.md)
* **[2024.04.25]** ğŸ¤— æˆ‘ä»¬åœ¨ Hugging Face Spaces ä¸Šå‘å¸ƒäº† [Open-Soraçš„Gradioæ¼”ç¤º](https://huggingface.co/spaces/hpcai-tech/open-sora)ã€‚
* **[2024.04.25]** æˆ‘ä»¬å‘å¸ƒäº†**Open-Sora 1.1**ï¼Œæ”¯æŒ**2s~15sã€144p åˆ° 720pã€ä»»æ„æ¯”ä¾‹çš„æ–‡æœ¬è½¬å›¾ç‰‡ã€æ–‡æœ¬è½¬è§†é¢‘ã€å›¾ç‰‡è½¬è§†é¢‘ã€è§†é¢‘è½¬è§†é¢‘ã€æ— é™æ—¶é—´ç”Ÿæˆ**ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†å®Œæ•´çš„è§†é¢‘å¤„ç†ç®¡é“ã€‚ [[checkpoints]]() [[report]](/docs/report_02.md)
* **[2024.03.18]** æˆ‘ä»¬å‘å¸ƒäº† **Open-Sora 1.0**, ä¸€ä¸ªå®Œå…¨å¼€æºçš„è§†é¢‘ç”Ÿæˆé¡¹ç›®ã€‚Open-Sora 1.0 æ”¯æŒå®Œæ•´çš„è§†é¢‘æ•°æ®é¢„å¤„ç†æµç¨‹ã€åŠ é€Ÿè®­ç»ƒ
  <a href="https://github.com/hpcaitech/ColossalAI"><img src="assets/readme/colossal_ai.png" width="8%" ></a>
ã€æ¨ç†ç­‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹åªéœ€ 3 å¤©çš„è®­ç»ƒå°±å¯ä»¥ç”Ÿæˆ 2 ç§’çš„ 512x512 è§†é¢‘ã€‚ [[checkpoints]](#open-sora-10-model-weights)
  [[blog]](https://hpc-ai.com/blog/open-sora-v1.0) [[report]](/docs/report_01.md)
* **[2024.03.04]** Open-Sora æä¾›åŸ¹è®­ï¼Œæˆæœ¬é™ä½ 46%ã€‚
  [[blog]](https://hpc-ai.com/blog/open-sora)

## ğŸ¥ Latest Demo

ğŸ”¥ æ‚¨å¯ä»¥åœ¨HuggingFaceä¸Šçš„ [ğŸ¤— Gradioåº”ç”¨ç¨‹åº](https://huggingface.co/spaces/hpcai-tech/open-sora)ä¸Šä½“éªŒOpen-Sora. æˆ‘ä»¬çš„[ç”»å»Š](https://hpcaitech.github.io/Open-Sora/)ä¸­æä¾›äº†æ›´å¤šç¤ºä¾‹.

<details>
<summary>OpenSora 1.1 æ¼”ç¤º</summary>

| **2ç§’ 240Ã—426**                                                                                                                                              | **2ç§’ 240Ã—426**                                                                                                                                             |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [<img src="assets/demo/sample_16x240x426_9.gif" width="">](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2) | [<img src="assets/demo/sora_16x240x426_26.gif" width="">](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2) |
| [<img src="assets/demo/sora_16x240x426_27.gif" width="">](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/f7ce4aaa-528f-40a8-be7a-72e61eaacbbd)  | [<img src="assets/demo/sora_16x240x426_40.gif" width="">](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/5d58d71e-1fda-4d90-9ad3-5f2f7b75c6a9) |

| **2ç§’ 426Ã—240**                                                                                                                                             | **4ç§’ 480Ã—854**                                                                                                                                              |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [<img src="assets/demo/sora_16x426x240_24.gif" width="">](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/34ecb4a0-4eef-4286-ad4c-8e3a87e5a9fd) | [<img src="assets/demo/sample_32x480x854_9.gif" width="">](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c1619333-25d7-42ba-a91c-18dbc1870b18) |

| **16ç§’ 320Ã—320**                                                                                                                                        | **16ç§’ 224Ã—448**                                                                                                                                        | **2ç§’ 426Ã—240**                                                                                                                                            |
| ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [<img src="assets/demo/sample_16s_320x320.gif" width="">](https://github.com/hpcaitech/Open-Sora/assets/99191637/3cab536e-9b43-4b33-8da8-a0f9cf842ff2) | [<img src="assets/demo/sample_16s_224x448.gif" width="">](https://github.com/hpcaitech/Open-Sora/assets/99191637/9fb0b9e0-c6f4-4935-b29e-4cac10b373c4) | [<img src="assets/demo/sora_16x426x240_3.gif" width="">](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/3e892ad2-9543-4049-b005-643a4c1bf3bf) |


</details>

<details>
<summary>OpenSora 1.0 Demo</summary>

| **2ç§’ 512Ã—512**                                                                                                                                                                 | **2ç§’ 512Ã—512**                                                                                                                                                              | **2ç§’ 512Ã—512**                                                                                                                                    |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| [<img src="assets/readme/sample_0.gif" width="">](https://github.com/hpcaitech/Open-Sora/assets/99191637/de1963d3-b43b-4e68-a670-bb821ebb6f80)                                 | [<img src="assets/readme/sample_1.gif" width="">](https://github.com/hpcaitech/Open-Sora/assets/99191637/13f8338f-3d42-4b71-8142-d234fbd746cc)                              | [<img src="assets/readme/sample_2.gif" width="">](https://github.com/hpcaitech/Open-Sora/assets/99191637/fa6a65a6-e32a-4d64-9a9e-eabb0ebb8c16)    |
|æ£®æ—åœ°åŒºå®é™çš„å¤œæ™¯ã€‚ [...] è¯¥è§†é¢‘æ˜¯ä¸€æ®µå»¶æ—¶æ‘„å½±ï¼Œæ•æ‰äº†ç™½å¤©åˆ°å¤œæ™šçš„è½¬å˜ï¼Œæ¹–æ³Šå’Œæ£®æ—å§‹ç»ˆä½œä¸ºèƒŒæ™¯ã€‚ | æ— äººæœºæ‹æ‘„çš„é•œå¤´æ•æ‰åˆ°äº†æµ·å²¸æ‚¬å´–çš„å£®ä¸½ç¾æ™¯ï¼Œ[...] æµ·æ°´è½»è½»åœ°æ‹æ‰“ç€å²©çŸ³åº•éƒ¨å’Œç´§è´´æ‚¬å´–é¡¶éƒ¨çš„ç»¿è‰²æ¤ç‰©ã€‚| ç€‘å¸ƒä»æ‚¬å´–ä¸Šå€¾æ³»è€Œä¸‹ï¼Œæµå…¥å®é™çš„æ¹–æ³Šï¼Œæ°”åŠ¿ç£…ç¤´ã€‚[...] æ‘„åƒæœºè§’åº¦æä¾›äº†ç€‘å¸ƒçš„é¸Ÿç°å›¾ã€‚ |
| [<img src="assets/readme/sample_3.gif" width="">](https://github.com/hpcaitech/Open-Sora/assets/99191637/64232f84-1b36-4750-a6c0-3e610fa9aa94)                                 | [<img src="assets/readme/sample_4.gif" width="">](https://github.com/hpcaitech/Open-Sora/assets/99191637/983a1965-a374-41a7-a76b-c07941a6c1e9)                              | [<img src="assets/readme/sample_5.gif" width="">](https://github.com/hpcaitech/Open-Sora/assets/99191637/ec10c879-9767-4c31-865f-2e8d6cf11e65)    |
| å¤œæ™šç¹åçš„åŸå¸‚è¡—é“ï¼Œå……æ»¡äº†æ±½è½¦å‰ç¯çš„å…‰èŠ’å’Œè·¯ç¯çš„æ°›å›´å…‰ã€‚ [...]                                                           | å‘æ—¥è‘µç”°çš„ç”Ÿæœºå‹ƒå‹ƒï¼Œç¾ä¸èƒœæ”¶ã€‚å‘æ—¥è‘µæ•´é½æ’åˆ—ï¼Œç»™äººä¸€ç§ç§©åºæ„Ÿå’Œå¯¹ç§°æ„Ÿã€‚ [...]                                            |å®é™çš„æ°´ä¸‹åœºæ™¯ï¼Œä¸€åªæµ·é¾Ÿåœ¨çŠç‘šç¤ä¸­æ¸¸åŠ¨ã€‚è¿™åªæµ·é¾Ÿçš„å£³å‘ˆç»¿è¤è‰² [...]                   |

è§†é¢‘ç»è¿‡é™é‡‡æ ·ä»¥.gifç”¨äºæ˜¾ç¤ºã€‚å•å‡»æŸ¥çœ‹åŸå§‹è§†é¢‘ã€‚æç¤ºç»è¿‡ä¿®å‰ªä»¥ç”¨äºæ˜¾ç¤ºï¼Œè¯·å‚é˜…[æ­¤å¤„](/assets/texts/t2v_samples.txt)æŸ¥çœ‹å®Œæ•´æç¤ºã€‚

</details>

## ğŸ”† æ–°åŠŸèƒ½/æ›´æ–°

* ğŸ“ **Open-Sora 1.2** å‘å¸ƒã€‚æ¨¡å‹æƒé‡å¯åœ¨[æ­¤å¤„](#model-weights)æŸ¥çœ‹ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„**[æŠ€æœ¯æŠ¥å‘Š v1.2](docs/report_03.md)** ã€‚ 
* âœ… æ”¯æŒæ•´æµæµè°ƒåº¦ã€‚
* âœ… è®­ç»ƒæˆ‘ä»¬çš„ 3D-VAE è¿›è¡Œæ—¶é—´ç»´åº¦å‹ç¼©ã€‚
* ğŸ“ **Open-Sora 1.1**å‘å¸ƒã€‚æ¨¡å‹æƒé‡å¯åœ¨[æ­¤å¤„](#model-weights)è·å¾—ã€‚å®ƒé’ˆå¯¹**0s~15sã€144p åˆ° 720pã€å„ç§å®½é«˜æ¯”**çš„è§†é¢‘è¿›è¡Œè®­ç»ƒã€‚æœ‰å…³æ›´å¤šè®¨è®ºï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„**[æŠ€æœ¯æŠ¥å‘Š v1.1](/docs/report_02.md)** ã€‚
* ğŸ”§ **æ•°æ®å¤„ç†æµç¨‹** v1.1å‘å¸ƒï¼Œæä¾›ä»åŸå§‹è§†é¢‘åˆ°ï¼ˆæ–‡æœ¬ï¼Œè§†é¢‘ç‰‡æ®µï¼‰å¯¹çš„è‡ªåŠ¨å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬åœºæ™¯å‰ªåˆ‡$\rightarrow$è¿‡æ»¤ï¼ˆç¾å­¦ã€å…‰æµã€OCR ç­‰ï¼‰$\rightarrow$å­—å¹•$\rightarrow$ç®¡ç†ã€‚ä½¿ç”¨æ­¤å·¥å…·ï¼Œæ‚¨å¯ä»¥è½»æ¾æ„å»ºè§†é¢‘æ•°æ®é›†ã€‚
* âœ… æ”¹è¿›çš„ ST-DiT æ¶æ„åŒ…æ‹¬ rope ä½ç½®ç¼–ç ã€qk èŒƒæ•°ã€æ›´é•¿çš„æ–‡æœ¬é•¿åº¦ç­‰ã€‚
* âœ… æ”¯æŒä»»æ„åˆ†è¾¨ç‡ã€çºµæ¨ªæ¯”å’Œæ—¶é•¿ï¼ˆåŒ…æ‹¬å›¾åƒï¼‰çš„è®­ç»ƒã€‚
* âœ… æ”¯æŒå›¾åƒå’Œè§†é¢‘è°ƒèŠ‚ä»¥åŠè§†é¢‘ç¼–è¾‘ï¼Œä»è€Œæ”¯æŒåŠ¨ç”»å›¾åƒï¼Œè¿æ¥è§†é¢‘ç­‰ã€‚
* ğŸ“ **Open-Sora 1.0**å‘å¸ƒã€‚æ¨¡å‹æƒé‡å¯åœ¨[æ­¤å¤„](#model-weights)è·å¾—ã€‚ä»…ä½¿ç”¨ 400K è§†é¢‘ç‰‡æ®µå’Œ 200 ä¸ª H800 å¤©ï¼ˆç›¸æ¯”ç¨³å®šè§†é¢‘æ‰©æ•£ä¸­çš„ 152M æ ·æœ¬ï¼‰ï¼Œæˆ‘ä»¬å°±èƒ½ç”Ÿæˆ 2s 512Ã—512 è§†é¢‘ã€‚æœ‰å…³æ›´å¤šè®¨è®ºï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„**[æŠ€æœ¯æŠ¥å‘Š v1.0](docs/report_01.md)**ã€‚
* âœ…ä»å›¾åƒæ‰©æ•£æ¨¡å‹åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ä¸‰é˜¶æ®µè®­ç»ƒã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªé˜¶æ®µæä¾›æƒé‡ã€‚
* âœ… æ”¯æŒè®­ç»ƒåŠ é€Ÿï¼ŒåŒ…æ‹¬åŠ é€Ÿ Transformerã€æ›´å¿«çš„ T5 å’Œ VAE ä»¥åŠåºåˆ—å¹¶è¡Œã€‚Open-Sora åœ¨ 64x512x512 è§†é¢‘ä¸Šè®­ç»ƒæ—¶å¯å°†è®­ç»ƒé€Ÿåº¦æé«˜**55%**ã€‚è¯¦ç»†ä¿¡æ¯ä½äº[è®­ç»ƒåŠ é€Ÿ.md](docs/acceleration.md)ã€‚
* ğŸ”§ **æ•°æ®é¢„å¤„ç†æµç¨‹ v1.0**,åŒ…æ‹¬ [ä¸‹è½½](tools/datasets/README.md), [è§†é¢‘å‰ªè¾‘](tools/scene_cut/README.md), å’Œ [å­—å¹•](tools/caption/README.md) å·¥å…·. æˆ‘ä»¬çš„æ•°æ®æ”¶é›†è®¡åˆ’å¯åœ¨ [æ•°æ®é›†.md](docs/datasets.md)ä¸­æ‰¾åˆ°.

<details>
<summary>æŸ¥çœ‹æ›´å¤š</summary>

âœ… æˆ‘ä»¬å‘ç°[VideoGPT](https://wilson1yan.github.io/videogpt/index.html)çš„ VQ-VAEè´¨é‡è¾ƒä½ï¼Œå› æ­¤é‡‡ç”¨äº†[Stability-AI](https://huggingface.co/stabilityai/sd-vae-ft-mse-original)ä¸­çš„æ›´å¥½çš„ VAE ã€‚æˆ‘ä»¬è¿˜å‘ç°æ—¶é—´ç»´åº¦çš„ä¿®è¡¥ä¼šé™ä½è´¨é‡ã€‚æœ‰å…³æ›´å¤šè®¨è®ºï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„**[æŠ€æœ¯æŠ¥å‘Šv1.0](docs/report_01.md)**ã€‚
âœ… æˆ‘ä»¬ç ”ç©¶äº†ä¸åŒçš„æ¶æ„ï¼ŒåŒ…æ‹¬ DiTã€Latte å’Œæˆ‘ä»¬æå‡ºçš„ **STDiT**ã€‚æˆ‘ä»¬çš„STDiTåœ¨è´¨é‡å’Œé€Ÿåº¦ä¹‹é—´å®ç°äº†æ›´å¥½çš„å¹³è¡¡ã€‚è¯·å‚é˜…æˆ‘ä»¬çš„ **[æŠ€æœ¯æŠ¥å‘Šv1.0](docs/report_01.md)**ä»¥äº†è§£æ›´å¤šè®¨è®ºã€‚
âœ… æ”¯æŒå‰ªè¾‘å’ŒT5æ–‡æœ¬è°ƒèŠ‚ã€‚
âœ… é€šè¿‡å°†å›¾åƒè§†ä¸ºå•å¸§è§†é¢‘ï¼Œæˆ‘ä»¬çš„é¡¹ç›®æ”¯æŒåœ¨å›¾åƒå’Œè§†é¢‘ä¸Šè®­ç»ƒ DiTï¼ˆä¾‹å¦‚ ImageNet å’Œ UCF101ï¼‰ã€‚æœ‰å…³æ›´å¤šè¯´æ˜ï¼Œè¯·å‚é˜…[commands.md](docs/commands.md) ã€‚
âœ… æ”¯æŒä½¿ç”¨[DiT](https://github.com/facebookresearch/DiT), [Latte](https://github.com/Vchitect/Latte),
  å’Œ [PixArt](https://pixart-alpha.github.io/).çš„å®˜æ–¹æƒé‡è¿›è¡Œæ¨ç†ã€‚
âœ… é‡æ„ä»£ç åº“ã€‚æŸ¥çœ‹[structure.md](docs/structure.md)ä»¥äº†è§£é¡¹ç›®ç»“æ„ä»¥åŠå¦‚ä½•ä½¿ç”¨é…ç½®æ–‡ä»¶ã€‚

</details>

### æŒ‰ä¼˜å…ˆçº§æ’åºçš„ TODO åˆ—è¡¨

<details>
<summary>æŸ¥çœ‹æ›´å¤š</summary>

* [x] è®­ç»ƒè§†é¢‘ VAE å¹¶ä½¿æˆ‘ä»¬çš„æ¨¡å‹é€‚åº”æ–°çš„ VAE
* [x] ç¼©æ”¾æ¨¡å‹å‚æ•°å’Œæ•°æ®é›†å¤§å°
* [x] çº³å…¥æ›´å¥½çš„è°ƒåº¦ç¨‹åºï¼ˆæ•´æµæµç¨‹ï¼‰
* [x] è¯„ä¼°æµç¨‹
* [x] å®Œæˆæ•°æ®å¤„ç†æµç¨‹ï¼ˆåŒ…æ‹¬å¯†é›†å…‰æµã€ç¾å­¦è¯„åˆ†ã€æ–‡æœ¬-å›¾åƒç›¸ä¼¼åº¦ç­‰ï¼‰ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[æ•°æ®é›†](/docs/datasets.md)
* [x] æ”¯æŒå›¾åƒå’Œè§†é¢‘è°ƒèŠ‚
* [x] æ”¯æŒå¯å˜çš„çºµæ¨ªæ¯”ã€åˆ†è¾¨ç‡å’ŒæŒç»­æ—¶é—´

</details>

## å†…å®¹

* [å®‰è£…](#å®‰è£…)
* [æ¨¡å‹æƒé‡](#æ¨¡å‹æƒé‡)
* [Gradioæ¼”ç¤º](#gradioæ¼”ç¤º)
* [æ¨ç†](#æ¨ç†)
* [æ•°æ®å¤„ç†](#æ•°æ®å¤„ç†)
* [è®­ç»ƒ](#è®­ç»ƒ)
* [è¯„ä¼°](#è¯„ä¼°)
* [è´¡çŒ®](#è´¡çŒ®)
* [å¼•ç”¨](#å¼•ç”¨)
* [è‡´è°¢](#è‡´è°¢)

ä¸‹é¢åˆ—å‡ºäº†å…¶ä»–æœ‰ç”¨çš„æ–‡æ¡£å’Œé“¾æ¥ã€‚

* æŠ¥å‘Š: [æŠ€æœ¯æŠ¥å‘Š v1.2](docs/report_03.md), [æŠ€æœ¯æŠ¥å‘Š v1.1](docs/report_02.md), [æŠ€æœ¯æŠ¥å‘Š v1.0](docs/report_01.md), [è®­ç»ƒåŠ é€Ÿ.md](docs/acceleration.md)
* Repo ç»“æ„: [ç»“æ„.md](docs/structure.md)
* é…ç½®æ–‡ä»¶è¯´æ˜: [config.md](docs/config.md)
* Useful commands: [commands.md](docs/commands.md)
* æ•°æ®å¤„ç†ç®¡é“å’Œæ•°æ®é›†: [datasets.md](docs/datasets.md)
* æ¯ä¸ªæ•°æ®å¤„ç†å·¥å…·çš„ README: [dataset conventions and management](/tools/datasets/README.md), [scene cutting](/tools/scene_cut/README.md), [scoring](/tools/scoring/README.md), [caption](/tools/caption/README.md)
* Evaluation: [eval](/eval/README.md)
* Gallery: [gallery](https://hpcaitech.github.io/Open-Sora/)

## å®‰è£…

### ä»æºå¤´å®‰è£…

For CUDA 12.1, you can install the dependencies with the following commands. Otherwise, please refer to [Installation](docs/installation.md) for more instructions on different cuda version, and additional dependency for data preprocessing.

```bash
# create a virtual env and activate (conda as an example)
conda create -n opensora python=3.9
conda activate opensora

# install torch, torchvision and xformers
pip install -r requirements/requirements-cu121.txt

# download the repo
git clone https://github.com/hpcaitech/Open-Sora
cd Open-Sora

# the default installation is for inference only
pip install -v . # for development mode, `pip install -v -e .`


(Optional, recommended for fast speed, especially for training) To enable `layernorm_kernel` and `flash_attn`, you need to install `apex` and `flash-attn` with the following commands.

```bash
# install flash attention
# set enable_flash_attn=False in config to disable flash attention
pip install packaging ninja
pip install flash-attn --no-build-isolation

# install apex
# set enable_layernorm_kernel=False in config to disable apex
pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings "--build-option=--cpp_ext" --config-settings "--build-option=--cuda_ext" git+https://github.com/NVIDIA/apex.git
```

### Use Docker

Run the following command to build a docker image from Dockerfile provided.

```bash
docker build -t opensora ./docker
```

Run the following command to start the docker container in interactive mode.

```bash
docker run -ti --gpus all -v {MOUNT_DIR}:/data opensora
```

## Model Weights

### Open-Sora 1.2 Model Weights

| Resolution | Model Size | Data | #iterations | Batch Size | URL |
| ---------- | ---------- | ---- | ----------- | ---------- | --- |
| TBD        |

See our **[report 1.2](docs/report_03.md)** for more infomation.

### Open-Sora 1.1 Model Weights

<details>
<summary>View more</summary>

| Resolution         | Model Size | Data                       | #iterations | Batch Size                                        | URL                                                                  |
| ------------------ | ---------- | -------------------------- | ----------- | ------------------------------------------------- | -------------------------------------------------------------------- |
| mainly 144p & 240p | 700M       | 10M videos + 2M images     | 100k        | [dynamic](/configs/opensora-v1-1/train/stage2.py) | [:link:](https://huggingface.co/hpcai-tech/OpenSora-STDiT-v2-stage2) |
| 144p to 720p       | 700M       | 500K HQ videos + 1M images | 4k          | [dynamic](/configs/opensora-v1-1/train/stage3.py) | [:link:](https://huggingface.co/hpcai-tech/OpenSora-STDiT-v2-stage3) |

See our **[report 1.1](docs/report_02.md)** for more infomation.

:warning: **LIMITATION**: This version contains known issues which we are going to fix in the next version (as we save computation resource for the next release). In addition, the video generation may fail for long duration, and high resolution will have noisy results due to this problem.

</details>

### Open-Sora 1.0 Model Weights

<details>
<summary>View more</summary>

| Resolution | Model Size | Data   | #iterations | Batch Size | GPU days (H800) | URL                                                                                           |
| ---------- | ---------- | ------ | ----------- | ---------- | --------------- |
| 16Ã—512Ã—512 | 700M       | 20K HQ | 20k         | 2Ã—64       | 35              | [:link:](https://huggingface.co/hpcai-tech/Open-Sora/blob/main/OpenSora-v1-HQ-16x512x512.pth) |
| 16Ã—256Ã—256 | 700M       | 20K HQ | 24k         | 8Ã—64       | 45              | [:link:](https://huggingface.co/hpcai-tech/Open-Sora/blob/main/OpenSora-v1-HQ-16x256x256.pth) |
| 16Ã—256Ã—256 | 700M       | 366K   | 80k         | 8Ã—64       | 117             | [:link:](https://huggingface.co/hpcai-tech/Open-Sora/blob/main/OpenSora-v1-16x256x256.pth)    |

Training orders: 16x256x256 $\rightarrow$ 16x256x256 HQ $\rightarrow$ 16x512x512 HQ.

Our model's weight is partially initialized from [PixArt-Î±](https://github.com/PixArt-alpha/PixArt-alpha). The number of
parameters is 724M. More information about training can be found in our **[report](/docs/report_01.md)**. More about
the dataset can be found in [datasets.md](/docs/datasets.md). HQ means high quality.

:warning: **LIMITATION**: Our model is trained on a limited budget. The quality and text alignment is relatively poor.
The model performs badly, especially on generating human beings and cannot follow detailed instructions. We are working
on improving the quality and text alignment.

</details>

## Gradio Demo

ğŸ”¥ You can experience Open-Sora on our [ğŸ¤— Gradio application](https://huggingface.co/spaces/hpcai-tech/open-sora) on Hugging Face online.

### Local Deployment

If you want to deploy gradio locally, we have also provided a [Gradio application](./gradio) in this repository, you can use the following the command to start an interactive web application to experience video generation with Open-Sora.

```bash
pip install gradio spaces
python gradio/app.py
```

This will launch a Gradio application on your localhost. If you want to know more about the Gradio applicaiton, you can refer to the [Gradio README](./gradio/README.md).

To enable prompt enhancement and other language input (e.g., ä¸­æ–‡è¾“å…¥), you need to set the `OPENAI_API_KEY` in the environment. Check [OpenAI's documentation](https://platform.openai.com/docs/quickstart) to get your API key.

```bash
export OPENAI_API_KEY=YOUR_API_KEY
```

### Getting Started

In the Gradio application, the basic options are as follows:

![Gradio Demo](assets/readme/gradio_basic.png)

The easiest way to generate a video is to input a text prompt and click the "**Generate video**" button (scroll down if you cannot find). The generated video will be displayed in the right panel. Checking the "**Enhance prompt with GPT4o**" will use GPT-4o to refine the prompt, while "**Random Prompt**" button will generate a random prompt by GPT-4o for you. Due to the OpenAI's API limit, the prompt refinement result has some randomness.

Then, you can choose the **resolution**, **duration**, and **aspect ratio** of the generated video. Different resolution and video length will affect the video generation speed. On a 80G H100 GPU, the generation speed and peak memory usage is:

|      | Image   | 2s       | 4s        | 8s        | 16s       |
| ---- | ------- | -------- | --------- | --------- | --------- |
| 360p | 3s, 24G | 18s, 27G | 31s, 27G  | 62s, 28G  | 121s, 33G |
| 480p | 2s, 24G | 29s, 31G | 55s, 30G  | 108s, 32G | 219s, 36G |
| 720p | 6s, 27G | 68s, 41G | 130s, 39G | 260s, 45G | 547s, 67G |

Note that besides text to video, you can also use image to video generation. You can upload an image and then click the "**Generate video**" button to generate a video with the image as the first frame. Or you can fill in the text prompt and click the "**Generate image**" button to generate an image with the text prompt, and then click the "**Generate video**" button to generate a video with the image generated with the same model.

![Gradio Demo](assets/readme/gradio_option.png)

Then you can specify more options, including "**Motion Strength**", "**Aesthetic**" and "**Camera Motion**". If "Enable" not checked or the choice is "none", the information is not passed to the model. Otherwise, the model will generate videos with the specified motion strength, aesthetic score, and camera motion.

For the **aesthetic score**, we recommend using values higher than 6. For **motion strength**, a smaller value will lead to a smoother but less dynamic video, while a larger value will lead to a more dynamic but likely more blurry video. Thus, you can try without it and then adjust it according to the generated video. For the **camera motion**, sometimes the model cannot follow the instruction well, and we are working on improving it.

You can also adjust the "**Sampling steps**", this is directly related to the generation speed as it is the number of denoising. A number smaller than 30 usually leads to a poor generation results, while a number larger than 100 usually has no significant improvement. The "**Seed**" is used for reproducibility, you can set it to a fixed number to generate the same video. The "**CFG Scale**" controls how much the model follows the text prompt, a smaller value will lead to a more random video, while a larger value will lead to a more text-following video (7 is recommended).

For more advanced usage, you can refer to [Gradio README](./gradio/README.md#advanced-usage).

## Inference

### Open-Sora 1.2 Command Line Inference

### GPT-4o Prompt Refinement

We find that GPT-4o can refine the prompt and improve the quality of the generated video. With this feature, you can also use other language (e.g., Chinese) as the prompt. To enable this feature, you need prepare your openai api key in the environment:

```bash
export OPENAI_API_KEY=YOUR_API_KEY
```

Then you can inference with `--llm-refine True` to enable the GPT-4o prompt refinement.

### Open-Sora 1.1 Command Line Inference

<details>
<summary>View more</summary>

Since Open-Sora 1.1 supports inference with dynamic input size, you can pass the input size as an argument.

```bash
# text to video
python scripts/inference.py configs/opensora-v1-1/inference/sample.py --prompt "A beautiful sunset over the city" --num-frames 32 --image-size 480 854
```

If your installation do not contain `apex` and `flash-attn`, you need to disable them in the config file, or via the folowing command.

```bash
python scripts/inference.py configs/opensora-v1-1/inference/sample.py --prompt "A beautiful sunset over the city" --num-frames 32 --image-size 480 854 --layernorm-kernel False --flash-attn False
```

See [here](docs/commands.md#inference-with-open-sora-11) for more instructions including text-to-image, image-to-video, video-to-video, and infinite time generation.

</details>

### Open-Sora 1.0 Command Line Inference

<details>
<summary>View more</summary>

We have also provided an offline inference script. Run the following commands to generate samples, the required model weights will be automatically downloaded. To change sampling prompts, modify the txt file passed to `--prompt-path`. See [here](docs/structure.md#inference-config-demos) to customize the configuration.

```bash
# Sample 16x512x512 (20s/sample, 100 time steps, 24 GB memory)
torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/16x512x512.py --ckpt-path OpenSora-v1-HQ-16x512x512.pth --prompt-path ./assets/texts/t2v_samples.txt

# Sample 16x256x256 (5s/sample, 100 time steps, 22 GB memory)
torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/16x256x256.py --ckpt-path OpenSora-v1-HQ-16x256x256.pth --prompt-path ./assets/texts/t2v_samples.txt

# Sample 64x512x512 (40s/sample, 100 time steps)
torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/64x512x512.py --ckpt-path ./path/to/your/ckpt.pth --prompt-path ./assets/texts/t2v_samples.txt

# Sample 64x512x512 with sequence parallelism (30s/sample, 100 time steps)
# sequence parallelism is enabled automatically when nproc_per_node is larger than 1
torchrun --standalone --nproc_per_node 2 scripts/inference.py configs/opensora/inference/64x512x512.py --ckpt-path ./path/to/your/ckpt.pth --prompt-path ./assets/texts/t2v_samples.txt
```

The speed is tested on H800 GPUs. For inference with other models, see [here](docs/commands.md) for more instructions.
To lower the memory usage, set a smaller `vae.micro_batch_size` in the config (slightly lower sampling speed).

</details>

## Data Processing

High-quality data is crucial for training good generation models.
To this end, we establish a complete pipeline for data processing, which could seamlessly convert raw videos to high-quality video-text pairs.
The pipeline is shown below. For detailed information, please refer to [data processing](docs/data_processing.md).
Also check out the [datasets](docs/datasets.md) we use.

![Data Processing Pipeline](assets/readme/report_data_pipeline.png)

## Training

### Open-Sora 1.2 Training

### Open-Sora 1.1 Training

<details>
<summary>View more</summary>

Once you prepare the data in a `csv` file, run the following commands to launch training on a single node.

```bash
# one node
torchrun --standalone --nproc_per_node 8 scripts/train.py \
    configs/opensora-v1-1/train/stage1.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT
# multiple nodes
colossalai run --nproc_per_node 8 --hostfile hostfile scripts/train.py \
    configs/opensora-v1-1/train/stage1.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT
```

</details>

### Open-Sora 1.0 Training

<details>
<summary>View more</summary>

Once you prepare the data in a `csv` file, run the following commands to launch training on a single node.

```bash
# 1 GPU, 16x256x256
torchrun --nnodes=1 --nproc_per_node=1 scripts/train.py configs/opensora/train/16x256x256.py --data-path YOUR_CSV_PATH
# 8 GPUs, 64x512x512
torchrun --nnodes=1 --nproc_per_node=8 scripts/train.py configs/opensora/train/64x512x512.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT
```

To launch training on multiple nodes, prepare a hostfile according
to [ColossalAI](https://colossalai.org/docs/basics/launch_colossalai/#launch-with-colossal-ai-cli), and run the
following commands.

```bash
colossalai run --nproc_per_node 8 --hostfile hostfile scripts/train.py configs/opensora/train/64x512x512.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT
```

For training other models and advanced usage, see [here](docs/commands.md) for more instructions.

</details>

## Evaluation

### VBench

### VBench-i2v

See [here](eval/README.md) for more instructions.

## Contribution

Thanks goes to these wonderful contributors:

<a href="https://github.com/hpcaitech/Open-Sora/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=hpcaitech/Open-Sora" />
</a>

If you wish to contribute to this project, please refer to the [Contribution Guideline](./CONTRIBUTING.md).

## Acknowledgement

Here we only list a few of the projects. For other works and datasets, please refer to our report.

* [ColossalAI](https://github.com/hpcaitech/ColossalAI): A powerful large model parallel acceleration and optimization
  system.
* [DiT](https://github.com/facebookresearch/DiT): Scalable Diffusion Models with Transformers.
* [OpenDiT](https://github.com/NUS-HPC-AI-Lab/OpenDiT): An acceleration for DiT training. We adopt valuable acceleration
  strategies for training progress from OpenDiT.
* [PixArt](https://github.com/PixArt-alpha/PixArt-alpha): An open-source DiT-based text-to-image model.
* [Latte](https://github.com/Vchitect/Latte): An attempt to efficiently train DiT for video.
* [StabilityAI VAE](https://huggingface.co/stabilityai/sd-vae-ft-mse-original): A powerful image VAE model.
* [CLIP](https://github.com/openai/CLIP): A powerful text-image embedding model.
* [T5](https://github.com/google-research/text-to-text-transfer-transformer): A powerful text encoder.
* [LLaVA](https://github.com/haotian-liu/LLaVA): A powerful image captioning model based on [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) and [Yi-34B](https://huggingface.co/01-ai/Yi-34B).
* [PLLaVA](https://github.com/magic-research/PLLaVA): A powerful video captioning model.
* [MiraData](https://github.com/mira-space/MiraData): A large-scale video dataset with long durations and structured caption.

We are grateful for their exceptional work and generous contribution to open source.

## Citation

```bibtex
@software{opensora,
  author = {Zangwei Zheng and Xiangyu Peng and Tianji Yang and Chenhui Shen and Shenggui Li and Hongxin Liu and Yukun Zhou and Tianyi Li and Yang You},
  title = {Open-Sora: Democratizing Efficient Video Production for All},
  month = {March},
  year = {2024},
  url = {https://github.com/hpcaitech/Open-Sora}
}
```

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=hpcaitech/Open-Sora&type=Date)](https://star-history.com/#hpcaitech/Open-Sora&Date)
